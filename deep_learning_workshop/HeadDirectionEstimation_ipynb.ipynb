{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7yCQH7obP9n"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydcD_hrBbzTo"
   },
   "source": [
    "###**Head pose estimation** \n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1sGZcJK_SKixlNjM-IeZSj-_D8yFB-B_L)\n",
    "\n",
    "\n",
    "**Head pose estimation** is a computer vision technique that determines the position and orientation of a person's head in a three-dimensional space using image or video data. It involves detecting and tracking facial landmarks to estimate the rotation and translation of the head. It has applications in areas such as human-computer interaction, augmented reality, driver monitoring systems, and more.\n",
    "\n",
    "\n",
    "\n",
    "## Area of usage\n",
    "\n",
    "1. Improved facial recognition\n",
    "\n",
    "2.   Surveillance and security\n",
    "\n",
    "3. Surveillance and security\n",
    "\n",
    "4. Gesture-based human-computer interaction\n",
    "\n",
    "5. Realistic computer graphics\n",
    "\n",
    "6. Immersive augmented reality\n",
    "\n",
    "7. Driver monitoring for safety\n",
    "\n",
    "8. Adaptive human-robot interaction\n",
    "\n",
    "##Main task\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1AwZUg4wosE4HA62JVmocPRdRAtbrfxGi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBFiJBlFija8"
   },
   "source": [
    "### Existing approaches\n",
    "\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1iA0DmiQFtTS6_nSNRLBzEWeZ7gLf9z0H)\n",
    "\n",
    "1.   Two-Step Approach (usage of keypoints)\n",
    "  \n",
    "\n",
    "*  Use a 2D facial landmark tracker like Facial Alignment Network (**FAN**) or **Dlib** to identify the landmarks in the image and produce their coordinates.\n",
    "*   Fit a 3D model of a human face to match these points to their 3D equivalents. A common approach is to fit a simple model of the mean human face. \n",
    "\n",
    "\n",
    "\n",
    "2.   One shot estimation\n",
    "*  Use single model to estimate head pose. State-of-art approaches: **WHEnet**, **6DepNet** \n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1A2eMLC6Vyybak723WeNOMKyiBthWoUah)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxZF_N26fAAO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUkzEjI3dhGD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs0lgghIl9I3"
   },
   "source": [
    "## To simplify task we focus on **YAW**, **PITCH** angles estimation\n",
    "\n",
    "Proposed method:\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1ZI8VjDf1BhG-lVMohnbvS60Qb06heJFZ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF8w3nEtmsfn"
   },
   "source": [
    "### Existing datasets\n",
    "\n",
    "\n",
    "\n",
    "1.   BIWI Kinect Head Pose\n",
    "2.   AFLW2000\n",
    "3.   300W_LP\n",
    "4.   AFL\n",
    "5. IBUG\n",
    "6. HELEN\n",
    "\n",
    "\n",
    "**300W_LP** dataset  contains almost all other datasets. We use it for training\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1vcVO81hH6hQJHDaiTD29XrCD_ziV4xLN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiTXEkrJoA04"
   },
   "source": [
    "## Main steps elements to run DL model for head pose prediction:\n",
    "\n",
    "*   dataset\n",
    "*   model\n",
    "*   train\n",
    "*   validation\n",
    "*   demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4IyNksGooV5"
   },
   "source": [
    "Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_zD-R-3o2DB"
   },
   "source": [
    "1. Need prepare utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement open4d (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for open4d\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "J3zW_6hNouTC",
    "outputId": "ca14ddc4-8def-45f8-bebc-34844d381e60"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-572a2b7ed482>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopen3d\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mo3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgui\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywavefront\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'open3d'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import open3d as o3d\n",
    "import open3d.visualization.gui as gui\n",
    "import pywavefront\n",
    "from astropy.coordinates import cartesian_to_spherical, spherical_to_cartesian\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYhkKEqmqGYO"
   },
   "source": [
    "Helper function to create pose from W300_LP mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wR_vd4B0qFXc"
   },
   "outputs": [],
   "source": [
    "def get_pose_W300_LP_from_mat(mat_path):\n",
    "    mat = sio.loadmat(mat_path)\n",
    "    # mat is constructed: pitch, yaw, roll, tdx, tdy, tdz, scale_factor\n",
    "    pre_pose_params = mat['Pose_Para'][0]\n",
    "    # Get pitch, yaw, roll\n",
    "    pose_params = pre_pose_params[:3]\n",
    "    return pose_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXgq_JJzqXn3"
   },
   "source": [
    "For visualization we use mesh in format obj. \n",
    "Read mesh using: \n",
    "```\n",
    "pywavefront\n",
    "```\n",
    "\n",
    "For 3d mesh representation it is nice to use lib:\n",
    "\n",
    "\n",
    "```\n",
    "open3d\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPJG60PKq9f3"
   },
   "outputs": [],
   "source": [
    "def setupMesh(path):\n",
    "    # Read 3d mesh from obj. file\n",
    "    meshObj = pywavefront.Wavefront(path, collect_faces=True)\n",
    "\n",
    "    # Build open3D mesh from pywavefront\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    vertices = np.array(meshObj.vertices, dtype=np.float32)\n",
    "    triangles = np.array(meshObj.meshes['eye_low_L_eyeball_mesh.002'].faces).astype(np.uint32)\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(triangles)\n",
    "    #Calculate normals for better visualization\n",
    "    mesh.compute_vertex_normals()\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJvVIwg9rDfH"
   },
   "source": [
    "We need to have helper function for conversion from:\n",
    "\n",
    "```\n",
    "yaw,pitch,roll -> direction vector â–¶ direction vector->yaw,pitch,roll\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w2vtC84rdZ1"
   },
   "outputs": [],
   "source": [
    "def convertAngle2Direction(yaw, pitch):\n",
    "    rho = 1.0\n",
    "    x, y, z = spherical_to_cartesian(rho, pitch, yaw)\n",
    "    return np.array([x, y, z], dtype=np.float32)\n",
    "\n",
    "def convertDirection2angle(x, y, z):\n",
    "    direction = np.array([x, y, z], dtype=np.float32)\n",
    "    normalized_direction = direction / np.linalg.norm(direction)\n",
    "    rho, theta, phi = cartesian_to_spherical(normalized_direction[0], normalized_direction[1], normalized_direction[2])\n",
    "    yaw = phi.radian * 180 / np.pi\n",
    "    pitch = theta.radian * 180 / np.pi\n",
    "    return yaw, pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJs87ZQkrkm0"
   },
   "source": [
    "One imortant thing - **visualization**\n",
    "\n",
    "For that we use **open3d** wrapped inside class **HeadPose3DVisualizer**\n",
    "\n",
    "We need to:\n",
    "\n",
    "*  Render angles\n",
    "*  Render directions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttTkmwlrsAUa"
   },
   "outputs": [],
   "source": [
    "class HeadPose3DVisualizer():\n",
    "    def __init__(self, w, h, mesh_path):\n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.data_mesh = setupMesh(mesh_path)\n",
    "\n",
    "        # Need to for 3d rendering\n",
    "        self.prev_yaw = 0\n",
    "        self.prev_pitch = 0\n",
    "        self.prev_roll = 0\n",
    "\n",
    "    def initMaterial(self):\n",
    "        # Initialize 3d mesh material\n",
    "        self.material = o3d.visualization.rendering.MaterialRecord()\n",
    "        self.material.base_color = (0.2, 0.3, 0.3, 1.0)\n",
    "        self.material.shader = \"defaultLit\"\n",
    "        self.bg_color = np.ndarray((4, 1), dtype=np.float32)\n",
    "\n",
    "    def renderDirection(self, x, y, z):\n",
    "        yaw, pitch = convertDirection2angle(x,y,z)\n",
    "        return self.renderAngles(yaw, pitch, 0.0)\n",
    "\n",
    "    def renderAngles(self, yaw, pitch, roll):\n",
    "        render = o3d.visualization.rendering.OffscreenRenderer(self.width, self.height)\n",
    "        self.initMaterial()\n",
    "        # Need to compensate rotation\n",
    "        cur_rotation = R.from_euler('yxz', [-self.prev_yaw, self.prev_pitch, -self.prev_roll], degrees=True)\n",
    "        self.data_mesh.rotate(cur_rotation.as_matrix(), center=(0, 0, 0))\n",
    "        self.data_mesh.rotate(R.from_euler('zxy', [roll, -pitch, yaw], degrees=True).as_matrix(), center=(0, 0, 0))\n",
    "\n",
    "        render.scene.scene.add_geometry(name=\"geo2\", geometry=self.data_mesh, material=self.material)\n",
    "        self.prev_yaw = yaw\n",
    "        self.prev_pitch = pitch\n",
    "        self.prev_roll = roll\n",
    "\n",
    "        #Camera setup\n",
    "        vertical_field_of_view = 75.0  # between 5 and 90 degrees\n",
    "        aspect_ratio = 1.0  # azimuth over elevation\n",
    "        near_plane = 0.10000000149011612\n",
    "        far_plane = 1.1079597473144531\n",
    "        fov_type = o3d.visualization.rendering.Camera.FovType.Horizontal\n",
    "        render.scene.camera.set_projection(vertical_field_of_view, aspect_ratio, near_plane, far_plane, fov_type)\n",
    "        center = [0, 0, 0]  # look_at target\n",
    "        eye = [0, 0.02, 0.33]  # camera position\n",
    "        up = [0, 1, 0]  # camera orientation\n",
    "        render.scene.camera.look_at(center, eye, up)\n",
    "        # render to image\n",
    "        img_o3d = render.render_to_image()\n",
    "        img_cv2 = cv2.cvtColor(np.array(img_o3d), cv2.COLOR_RGBA2BGRA)\n",
    "\n",
    "        return img_cv2[:, :, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OO49FQSasSIt"
   },
   "source": [
    "Now we can switch to the **dataset**\n",
    "\n",
    "For that  **torch** has nice classes:\n",
    "\n",
    "\n",
    "```\n",
    "torch.utils.data.dataset.Dataset\n",
    "torch.utils.data.DataLoader\n",
    "torchvision.transforms\n",
    "```\n",
    "\n",
    "Firstly implement basic class inhereted from `torch.utils.data.dataset.Dataset`\n",
    "For that we need implement several methods: `__getitem__`,  `__len__`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxqMoE9Hthm-"
   },
   "outputs": [],
   "source": [
    "def getData(path):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if '.mat' not in file:\n",
    "                mat_path = os.path.join(root, file.split('.')[0])\n",
    "                img_path = os.path.join(root, file)\n",
    "                x_data.append(img_path)\n",
    "                y_data.append(mat_path)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "class PoseDataW300_LP(Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.x_data, self.y_data = getData(data_dir)\n",
    "        self.data_length = len(self.x_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-ggF_0Eti5V"
   },
   "source": [
    "Method `__getitem__` and `__len__`\n",
    "Inside `__getitem__` we read our data: image and mat label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Otz47S16uA65"
   },
   "outputs": [],
   "source": [
    "    def __getitem__(self, index):\n",
    "        # Load image\n",
    "        img = io.imread(self.x_data[index])\n",
    "        # Load label\n",
    "        mat_path =self.y_data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feJ8F7x-uFNC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S59owV9uuBvj"
   },
   "source": [
    "Now using ```get_pose_W300_LP_from_mat``` we get our pose.\n",
    "With  ```convertAngle2Direction(yaw, pitch)``` we transform angles to direction vector\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIsCPdOVsRfS"
   },
   "outputs": [],
   "source": [
    "\n",
    "        # Get pitch, yaw, roll from mat\n",
    "        pose = get_pose_W300_LP_from_mat(mat_path)\n",
    "        # Convert radians to degrees.\n",
    "        pitch = pose[0]\n",
    "        yaw = -pose[1]\n",
    "        roll = pose[2]\n",
    "\n",
    "        #Convert yaw, pitch to direction vector\n",
    "        direction_vector = convertAngle2Direction(yaw, pitch)\n",
    "        sample = {'image': img, 'direction': direction_vector}\n",
    "\n",
    "        if self.transform is not None:\n",
    "            return self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEFTHCCQusvW"
   },
   "source": [
    "Now we have our own data class. \n",
    "\n",
    "But how about adding some augmentation?\n",
    "Here we  can use `Rotation` augmentation. \n",
    "Each augmentation should have implemented ```__call__``` method\n",
    "\n",
    "We would like to rotate image + direction vector into angle from some range **[minAngle, maxAngle]**. For example better to use **[-30, 30]** degrees\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QgSI9GLvseD"
   },
   "outputs": [],
   "source": [
    "class Rotate(object):\n",
    "    #Rotate image with direction vector\n",
    "    def __init__(self, angle_range):\n",
    "        self.angle_range = angle_range\n",
    "        assert isinstance(angle_range, (float, tuple))\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, direction = sample['image'], sample['direction']\n",
    "\n",
    "        angle = random.randint(self.angle_range[0], self.angle_range[1])\n",
    "        rotater = transforms.RandomRotation(degrees=[angle, angle])\n",
    "        yaw, pitch = convertDirection2angle(direction[0], direction[1], direction[2])\n",
    "        rotation = RotationObject.from_euler('zxy', [0.0, -pitch, yaw], degrees=True)\n",
    "        vect = [0, 0, -angle * np.pi / 180.0]\n",
    "\n",
    "        newRotation = RotationObject.from_rotvec(vect)\n",
    "        mainRotation = newRotation.as_matrix().dot(rotation.as_matrix())\n",
    "        matrixRot = RotationObject.from_matrix(mainRotation)\n",
    "\n",
    "        processedAngles = matrixRot.as_euler(seq='zxy', degrees=True)\n",
    "        new_yaw = processedAngles[2] * np.pi/180.0\n",
    "        new_pitch = processedAngles[1] * np.pi/180.0\n",
    "        return {'image': rotater(image), 'direction': convertAngle2Direction(new_yaw, new_pitch)}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYMF97Yav3e2"
   },
   "source": [
    "Transforms numpy image, tuple -> tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ryl7jap2wGYS"
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['direction']\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'direction': torch.from_numpy(landmarks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA2NVsQkwWeq"
   },
   "source": [
    "Now we are ready to run dataset generation and see how it looks like\n",
    "For that we need:\n",
    "\n",
    "\n",
    "1.   Create our dataset object:```PoseDataW300_LP```\n",
    "\n",
    "2.   Add data loader for generating batches ```DataLoader```\n",
    "3. Add visualization with ```HeadDirectionVisualizer```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr-RW5mywVwQ"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    head_pose_dataset = PoseDataW300_LP(data_dir='D:/Workshop/Dataset/300W_LP/AFW',\n",
    "                                        transform =transforms.Compose([Rotate((-30, 30)),ToTensor()]))\n",
    "\n",
    "    visualizer = HeadDirectionVisualizer(1024, 512, \"D:/Workshop/mesh/mesh.obj\")\n",
    "\n",
    "    dataloader = DataLoader(head_pose_dataset, batch_size=4,\n",
    "                            shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        print(i_batch, sample_batched['image'].size(), sample_batched['direction'].size())\n",
    "        image = sample_batched['image'][0]\n",
    "        direction = sample_batched['direction'][0].detach().cpu().numpy()\n",
    "        np_image = np.array(image)[..., :3]\n",
    "        img = o3d.geometry.Image(np_image)\n",
    "        result = visualizer.showDirection(img, direction[0], direction[1], direction[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ0Qx3wQysZk"
   },
   "source": [
    "\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1nMk6KwH0R_Gkve8qtw8X4MktoXHNIpMn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mGBBmbRy9Ef"
   },
   "source": [
    "## Now we can switch to the model creation\n",
    "As basic model we will use AlexNet NN with several modifications:\n",
    "\n",
    "\n",
    "1.   Add  `nn.AdaptiveAvgPool2d` at the and of feature extractior\n",
    "2.  Regression layers will have less parameters **4096 -> 256**\n",
    "3.  Last layer has tensor with 3 velues - **direction vector**\n",
    "\n",
    "![picture](https://drive.google.com/uc?export=view&id=1jmLuDlA9VifWzUtH6zn5DRIj_DRUsAcD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqm6HosZ1aPl"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjDapaGny3BA"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class HeadPOseEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # output: 128 x 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2, 2),  # output: 256 x 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # 256 x 1 x 1\n",
    "            )\n",
    "\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Linear(256, 3))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.regression(h)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhkQvlex1kKe"
   },
   "source": [
    "When we have a **model** and **data** - need to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTlCJJXa1wpO"
   },
   "source": [
    "To compare 2 direction vectors better to use **CosineSimilarity** instead of **MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbjTIx-N1-hT"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "regression_loss = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "outputs = torch.rand(4, 3)\n",
    "gt_data = torch.rand(4, 3)\n",
    "\n",
    "\n",
    "print(outputs)\n",
    "print(gt_data)\n",
    "\n",
    "loss = regression_loss(outputs, gt_data)\n",
    "print('Total loss for batch: {}'.format(torch.mean(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChcEQXBZ2lzY"
   },
   "source": [
    "Need to define optimizar. As mentioned before - we can use` Momentum,Adam, RMSProp` and other optimizers. In our case we will use **Adam** optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PoDV5aB2lYa"
   },
   "outputs": [],
   "source": [
    "    model = HeadPOseEstimator()\n",
    "\n",
    "    # Optimizers specified in the torch.optim package\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RLBQ0ef3fBs"
   },
   "source": [
    "Training loop\n",
    "\n",
    "1. `DataLoader`->Data\n",
    "\n",
    "2. Zeros the optimizerâ€™s gradients\n",
    "\n",
    "3. Inference on model\n",
    "\n",
    "4. Calculates the loss for that set of predictions vs. the labels on the dataset\n",
    "\n",
    "5. Calculates the backward gradients over the learning weights\n",
    "\n",
    "6. Conduct one learning step\n",
    "\n",
    "7. Report loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnbdXjKU3eiN"
   },
   "outputs": [],
   "source": [
    "    head_pose_dataset_train = PoseDataW300_LP(data_dir='D:/Workshop/Dataset/300W_LP/LFPW',\n",
    "                                        transform =transforms.Compose([ToTensor()]))\n",
    "\n",
    "    head_pose_dataset_validation = PoseDataW300_LP(data_dir='D:/Workshop/Dataset/300W_LP/IBUG',\n",
    "                                        transform=transforms.Compose([ToTensor()]))\n",
    "\n",
    "    visualizer = HeadDirectionVisualizer(1024, 512, \"D:/Workshop/mesh/mesh.obj\")\n",
    "\n",
    "    dataloader_train = DataLoader(head_pose_dataset_train, batch_size=16,\n",
    "                            shuffle=True, num_workers=4)\n",
    "\n",
    "    dataloader_validation = DataLoader(head_pose_dataset_validation, batch_size=16,\n",
    "                                  shuffle=True, num_workers=4)\n",
    "\n",
    "    def train_one_epoch(epoch_index, tb_writer):\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        for i, data in enumerate(dataloader_train):\n",
    "            inputs, directions = data\n",
    "            # Zero your gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Make predictions\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_function(outputs, directions)\n",
    "            loss.backward()\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                last_loss = running_loss / 1000  # loss per batch\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                tb_x = epoch_index * len(dataloader) + i + 1\n",
    "                tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                running_loss = 0.\n",
    "\n",
    "        return last_loss\n",
    "\n",
    "\n",
    "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "\n",
    "    EPOCHS = 5\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(dataloader_validation):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = regression_loss(voutputs, vlabels)\n",
    "                running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                           {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                           epoch_number + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
